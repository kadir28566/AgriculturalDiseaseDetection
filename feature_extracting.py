# -*- coding: utf-8 -*-
"""Feature Extracting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dOfVL8pM1NiwAcaEzH3IQ3lNS-WOYjUW
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!pip install pyspark

import cv2 as cv
import numpy as np
from skimage.feature import graycomatrix, graycoprops
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, FloatType, StringType

# Spark oturumunu başlat
spark = SparkSession.builder.appName("ImageFeatureExtraction").getOrCreate()

@udf(returnType=ArrayType(FloatType()))
def extract_all_features_udf(image_path):
    def extract_color_features(image):
        color_features = cv.mean(image)[:3]
        hsv_images = cv.cvtColor(image, cv.COLOR_BGR2HSV)
        hsv_features = cv.mean(hsv_images)[:3]
        return np.concatenate([color_features, hsv_features])

    def extract_glcm_features(image):
        gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)
        glcm = graycomatrix(gray, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4])
        contrast = graycoprops(glcm, 'contrast')
        dissimilarity = graycoprops(glcm, 'dissimilarity')
        homogeneity = graycoprops(glcm, 'homogeneity')
        energy = graycoprops(glcm, 'energy')
        correlation = graycoprops(glcm, 'correlation')
        return np.concatenate([contrast, dissimilarity, homogeneity, energy, correlation], axis=0)

    def extract_edge_features(image):
        gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)
        edges = cv.Canny(gray, 100, 200)
        edge_density = np.mean(edges/255.0)
        return edge_density

    image = cv.imread(image_path)
    if image is not None:
        color_features = extract_color_features(image)
        glcm_features = extract_glcm_features(image)
        edge_features = extract_edge_features(image)

        color_features = np.ravel(color_features)
        glcm_features = np.ravel(glcm_features)
        edge_features = np.ravel([edge_features])

        all_features = np.concatenate([color_features, glcm_features, edge_features])
        return all_features.tolist()
    return None

def process_dataset_spark(dataset_path):
    # Görüntü yollarını ve etiketlerini topla
    image_data = []
    for class_name in os.listdir(dataset_path):
        class_path = os.path.join(dataset_path, class_name)
        if os.path.isdir(class_path):
            for image_name in os.listdir(class_path):
                image_path = os.path.join(class_path, image_name)
                image_data.append((image_path, class_name))

    # DataFrame oluştur
    df = spark.createDataFrame(image_data, ["image_path", "label"])

    # Özellikleri çıkar
    df_with_features = df.withColumn("features", extract_all_features_udf(df.image_path))

    return df_with_features

dataset_path = "/content/drive/MyDrive/Apple"


try:
  # Veri setini işle
  result_df = process_dataset_spark(dataset_path)
except Exception as e:
  print(f"Hata oluştu: {e}")

# Sonuçları topla
collected_results = result_df.collect()

# NumPy dizilerine dönüştür
X = np.array([row.features for row in collected_results if row.features is not None])
y = np.array([row.label for row in collected_results if row.features is not None])




print("Özellik matrisinin boyutu: ", X.shape)
print("Etiket vektörünün boyutu: ", y.shape)

# Spark oturumunu kapat
spark.stop()

np.savetxt("X_features.txt", X, delimiter=",", fmt="%.5f")
np.savetxt("y_labels.txt", y, fmt="%s")

