# -*- coding: utf-8 -*-
"""Data_Processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xCpuCZ5EZE4zrooj5WLGW4Xy_qFss54c
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!pip install pyspark

import cv2 as cv
from google.colab.patches import cv2_imshow
import os
from pyspark.sql import SparkSession
import csv

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["PATH"] = os.environ["JAVA_HOME"] + "/bin:" + os.environ["PATH"]

from pyspark.sql import SparkSession

# Spark oturumu oluştur
spark = SparkSession.builder.master("local[*]").getOrCreate()

# Spark sürümünü yazdır
print("PySpark sürümü:", spark.version)

# Veri Artırma

folder_path = "/content/drive/MyDrive/Apple/Apple healthy"

print(len(os.listdir(folder_path)))

def process_image(files):

  for file_path in files:

    img = cv.imread(file_path)

    if img is not None:
      rotated90_img = cv.rotate(img, cv.ROTATE_90_CLOCKWISE)
      counterRotated90_img = cv.rotate(img, cv.ROTATE_90_COUNTERCLOCKWISE)
      rotated180_img = cv.rotate(img, cv.ROTATE_180)

      filename = os.path.basename(file_path)

      cv.imwrite(os.path.join(folder_path, f"{filename}+rotated90.jpg"), rotated90_img)
      cv.imwrite(os.path.join(folder_path, f"{filename}+counterRotated90.jpg"), counterRotated90_img)
      cv.imwrite(os.path.join(folder_path, f"{filename}+rotated180.jpg"), rotated180_img)


file_list = os.listdir(folder_path)
file_paths = [os.path.join(folder_path, filename) for filename in file_list]

rdd = spark.sparkContext.parallelize(file_paths)
rdd.foreachPartition(process_image)

spark.stop()

print(len(os.listdir(folder_path)))

from pyspark.sql import SparkSession

# Spark oturumu oluştur
spark = SparkSession.builder.master("local[*]").getOrCreate()

# Spark sürümünü yazdır
print("PySpark sürümü:", spark.version)
# Veri Artırma

folder_path = "/content/drive/MyDrive/Apple/Apple Cedar apple rust (sedir elma pasi)"

print(len(os.listdir(folder_path)))

def process_image(files):

  for file_path in files:

    img = cv.imread(file_path)

    if img is not None:
      rotated90_img = cv.rotate(img, cv.ROTATE_90_CLOCKWISE)
      counterRotated90_img = cv.rotate(img, cv.ROTATE_90_COUNTERCLOCKWISE)
      rotated180_img = cv.rotate(img, cv.ROTATE_180)

      filename = os.path.basename(file_path)

      cv.imwrite(os.path.join(folder_path, f"{filename}+rotated90.jpg"), rotated90_img)
      cv.imwrite(os.path.join(folder_path, f"{filename}+counterRotated90.jpg"), counterRotated90_img)
      cv.imwrite(os.path.join(folder_path, f"{filename}+rotated180.jpg"), rotated180_img)


file_list = os.listdir(folder_path)
file_paths = [os.path.join(folder_path, filename) for filename in file_list]

rdd = spark.sparkContext.parallelize(file_paths)
rdd.foreachPartition(process_image)

spark.stop()

print(len(os.listdir(folder_path)))

from pyspark.sql import SparkSession

# Spark oturumu oluştur
spark = SparkSession.builder.master("local[*]").getOrCreate()

# Spark sürümünü yazdır
print("PySpark sürümü:", spark.version)
# Veri Artırma

folder_path = "/content/drive/MyDrive/Apple/Apple Apple scab (elma uyuzu)"

print(len(os.listdir(folder_path)))

def process_image(files):

  for file_path in files:

    img = cv.imread(file_path)

    if img is not None:
      rotated90_img = cv.rotate(img, cv.ROTATE_90_CLOCKWISE)
      counterRotated90_img = cv.rotate(img, cv.ROTATE_90_COUNTERCLOCKWISE)
      rotated180_img = cv.rotate(img, cv.ROTATE_180)

      filename = os.path.basename(file_path)

      cv.imwrite(os.path.join(folder_path, f"{filename}+rotated90.jpg"), rotated90_img)
      cv.imwrite(os.path.join(folder_path, f"{filename}+counterRotated90.jpg"), counterRotated90_img)
      cv.imwrite(os.path.join(folder_path, f"{filename}+rotated180.jpg"), rotated180_img)


file_list = os.listdir(folder_path)
file_paths = [os.path.join(folder_path, filename) for filename in file_list]

rdd = spark.sparkContext.parallelize(file_paths)
rdd.foreachPartition(process_image)

spark.stop()

print(len(os.listdir(folder_path)))

from pyspark.sql import SparkSession

# Spark oturumu oluştur
spark = SparkSession.builder.master("local[*]").getOrCreate()

# Spark sürümünü yazdır
print("PySpark sürümü:", spark.version)
# Veri Artırma

folder_path = "/content/drive/MyDrive/Apple/Apple Black rot (siyah curuk)"

print(len(os.listdir(folder_path)))

def process_image(files):

  for file_path in files:

    img = cv.imread(file_path)

    if img is not None:
      rotated90_img = cv.rotate(img, cv.ROTATE_90_CLOCKWISE)
      counterRotated90_img = cv.rotate(img, cv.ROTATE_90_COUNTERCLOCKWISE)
      rotated180_img = cv.rotate(img, cv.ROTATE_180)

      filename = os.path.basename(file_path)

      cv.imwrite(os.path.join(folder_path, f"{filename}+rotated90.jpg"), rotated90_img)
      cv.imwrite(os.path.join(folder_path, f"{filename}+counterRotated90.jpg"), counterRotated90_img)
      cv.imwrite(os.path.join(folder_path, f"{filename}+rotated180.jpg"), rotated180_img)


file_list = os.listdir(folder_path)
file_paths = [os.path.join(folder_path, filename) for filename in file_list]

rdd = spark.sparkContext.parallelize(file_paths)
rdd.foreachPartition(process_image)

spark.stop()

print(len(os.listdir(folder_path)))

# Spark oturumu oluştur
spark = SparkSession.builder.master("local[*]").getOrCreate()

# Spark sürümünü yazdır
print("PySpark sürümü:", spark.version)
# Veri Artırma

def makegray(files):

  for file_path in files:

    img = cv.imread(file_path)

    if img is not None:


      gray_img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)

      filename = os.path.basename(file_path)

      cv.imwrite(os.path.join(folder, f"{filename}"+"_gray.jpg"), gray_img)


folder_path = [os.path.join("/content/drive/MyDrive/Apple", folder) for folder in os.listdir("/content/drive/MyDrive/Apple/") if folder != ".ipynb_checkpoints"]

for folder in folder_path:
  print(folder + " boyutu: " + str(len(os.listdir(folder))))

  file_list = os.listdir(folder)
  file_paths = [os.path.join(folder, filename) for filename in file_list]

  rdd = spark.sparkContext.parallelize(file_paths)
  rdd.foreachPartition(makegray)
  print(folder + " boyutu: " + str(len(os.listdir(folder))))

spark.stop()

# Spark oturumu oluştur
spark = SparkSession.builder.master("local[*]").getOrCreate()

# Spark sürümünü yazdır
print("PySpark sürümü:", spark.version)
# Veri Artırma

def makegray(files):

  for file_path in files:

    img = cv.imread(file_path)

    if img is not None:


      gray_img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)

      filename = os.path.basename(file_path)

      cv.imwrite(os.path.join(folder, f"{filename}"+"_gray.jpg"), gray_img)


folder_path = [os.path.join("/content/drive/MyDrive/Apple", folder) for folder in os.listdir("/content/drive/MyDrive/Apple/") if folder == "Apple Black rot (siyah curuk)"]

for folder in folder_path:
  print(folder + " boyutu: " + str(len(os.listdir(folder))))

  file_list = os.listdir(folder)
  file_paths = [os.path.join(folder, filename) for filename in file_list]

  rdd = spark.sparkContext.parallelize(file_paths)
  rdd.foreachPartition(makegray)
  print(folder + " boyutu: " + str(len(os.listdir(folder))))

spark.stop()

folder_path = "/content/drive/MyDrive/Apple/Apple healthy"
folder_path2 = "/content/drive/MyDrive/Apple/Apple Apple scab (elma uyuzu)"
folder_path3 = "/content/drive/MyDrive/Apple/Apple Black rot (siyah curuk)"
folder_path4 = "/content/drive/MyDrive/Apple/Apple Cedar apple rust (sedir elma pasi)"

output_file = "labeled.csv"
output_file2 = "labeled_scab.csv"
output_file3 = "labeled_blackRot.csv"
output_file4= "labeled_cedarAppleRust.csv"


with open (output_file, "w", encoding="utf-8") as f:
  for filename in os.listdir(folder_path):
    f.write(filename + "," + " " + "Healthy" + "\n")
print(f"dosya işlemleri {output_file} dosyasına yazıldı.")

with open (output_file2, "w", encoding="utf-8") as f:
  for filename in os.listdir(folder_path2):
    f.write(filename + "," + " " + "Apple Scab" + "\n")
print(f"dosya işlemleri {output_file2} dosyasına yazıldı.")

with open (output_file3, "w", encoding="utf-8") as f:
  for filename in os.listdir(folder_path3):
    f.write(filename + "," + " " + "Black Rot" + "\n")
print(f"dosya işlemleri {output_file3} dosyasına yazıldı.")

with open (output_file4, "w", encoding="utf-8") as f:
  for filename in os.listdir(folder_path4):
    f.write(filename + "," + " " + "Cedar Apple Rust" + "\n")
print(f"dosya işlemleri {output_file4} dosyasına yazıldı.")

def SplitDataset(file):



  df = spark.read.csv(os.path.join("/content/csvs", file), header=False, inferSchema=True)

  traindf, testdf = df.randomSplit([0.7, 0.3], seed=123)

  traindf.write.csv(os.path.join("/content/train_csv", f"{file}_train.csv"), header=False, mode="overwrite")
  testdf.write.csv(os.path.join("/content/train_csv", f"{file}_test.csv"), header=False, mode="overwrite")



  print(f"{file} dosyası işlendi.")

spark = SparkSession.builder.master("local[*]").getOrCreate()
files = [file for file in os.listdir("/content/csvs")]
for file in files:
  SplitDataset(file)
spark.stop()

